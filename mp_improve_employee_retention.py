# -*- coding: utf-8 -*-
"""MP-Improve Employee Retention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VdcjKbs2ofLStHJSzBmuGlEfPl3H5VFq
"""

!pip install shap

!pip install catboost

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
from matplotlib import rcParams
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
from scipy import stats
from datetime import date
import matplotlib.dates as mdates
from datetime import datetime, timedelta

import time
from sklearn.model_selection import train_test_split,cross_validate

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.model_selection import cross_validate
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve
from sklearn.metrics import confusion_matrix
from sklearn import metrics

import warnings
warnings.filterwarnings('ignore')

dfraw = pd.read_excel('https://drive.google.com/uc?export=download&id=1xHtu7PSm4w6EfJyt2cRiftgcEWnxr_8g')
dfraw.info()
pd.set_option('display.max_columns', None)
dfraw.sample(5)

# Check duplicated rows
dfraw.duplicated().sum()

# Checking null values
null_cols = dfraw.columns[dfraw.isnull().any()]
null_val = dfraw[null_cols].isnull().sum().to_frame().reset_index()
null_val.columns = ['Column', 'Count']
null_val['Percent%'] = round(null_val['Count']/len(dfraw) * 100, 3)
null_val

dfraw.describe().T

dfraw.describe(include="object").T

"""# EDA

## Unique Value
"""

dfraw['StatusPernikahan'].unique()

# Rows with nan value in 'Status Pernikahan'
dfraw[dfraw['StatusPernikahan'] == '-']

dfraw['StatusKepegawaian'].unique()

dfraw['Pekerjaan'].unique()

dfraw['PernahBekerja'].unique()

dfraw['JenjangKarir'].unique()

dfraw['PerformancePegawai'].unique()

dfraw['AsalDaerah'].unique()

dfraw['HiringPlatform'].unique()

dfraw['TingkatPendidikan'].unique()

dfraw['AlasanResign'].unique()

"""Terlihat sepertinya semua nilai kolom kategorikal sudah sesuai, kecuali pada kolom `AlasanResign`. Selain memiliki null value di dalamnya, ada value yang tidak sesuai yaitu "Product Design" dimana seharusnya menjadi value pada kolom `Pekerjaan`. Karena abnormal value hanya 4 baris maka baris ini akan di drop nantinya."""

# Rows with abnormal value in 'Alasan Resign'
dfraw[dfraw['AlasanResign'] == 'Product Design (UI & UX)']

# Graph of Data Distribution
num = ['EnterpriseID', 'SkorSurveyEngagement', 'SkorKepuasanPegawai', 'JumlahKeikutsertaanProjek', 'JumlahKeterlambatanSebulanTerakhir', 'JumlahKetidakhadiran']

plt.figure(figsize=(10, 4))
for i in range(0, len(num)):
    plt.subplot(2, 3, i+1)
    sns.distplot(dfraw[num[i]], color='blue')
    plt.tight_layout()

# Checking Outliers
plt.figure(figsize=(10, 4))
for i in range(0, len(num)):
    plt.subplot(2, 3,  i+1)
    sns.boxplot(dfraw[num[i]], color='red')
    plt.title(num[i])
    plt.tight_layout()

cat = ['StatusPernikahan','JenisKelamin','StatusKepegawaian','Pekerjaan','JenjangKarir','PerformancePegawai','AsalDaerah','HiringPlatform',
       'TingkatPendidikan','AlasanResign','IkutProgramLOP']

plt.figure(figsize=(7,20))
for i in range(len(cat)):
    plt.subplot( 11,1, i+1 )
    sns.countplot( data=dfraw, y=dfraw[cat[i]] )
    plt.tick_params(axis='both', which='major', labelsize=7)
    plt.tight_layout()

"""# Data Cleansing

### Dropping
"""

dfclean = dfraw.copy()

# Drop columns
dfclean = dfclean.drop(columns=['PernahBekerja','Email','NomorHP','IkutProgramLOP'])
dfclean.shape

"""### Handling Missing Values and Replace Invalid Values

- SkorKepuasanPegawai = 1.742 %    
- JumlahKeikutsertaanProjek = 1.045%  
- JumlahKeterlambatanSebulanTerakhir = 0.348%  
- JumlahKetidakhadiran = 2.091 %  
- IkutProgramLOP = 89.89 %  (Has been deleted)
- AlasanResign = 22.997 %
- StatusPernikahan = 3 rows (denoetd as nan)
"""

# Handling missing values

# Replace Invalid Values
dfclean['StatusPernikahan'].replace('-', 'Lainnya', inplace=True)
dfclean['AlasanResign'].replace('Product Design (UI & UX)', 'Lainnya', inplace=True)

# Missing value in the reason for resign means they still working in company
dfclean['AlasanResign'] = dfclean['AlasanResign'].fillna('masih_bekerja')

# Fill numerical columns with median
dfclean['JumlahKeikutsertaanProjek'].fillna(dfclean['JumlahKeikutsertaanProjek'].median(), inplace=True)
dfclean['JumlahKeterlambatanSebulanTerakhir'].fillna(dfclean['JumlahKeterlambatanSebulanTerakhir'].median(), inplace=True)
dfclean['JumlahKetidakhadiran'].fillna(dfclean['JumlahKetidakhadiran'].median(), inplace=True)

# Fill other columns
dfclean['StatusPernikahan'] = dfclean['StatusPernikahan'].fillna('Lainnya')
dfclean['SkorKepuasanPegawai'].fillna(dfclean['SkorKepuasanPegawai'].mode()[0], inplace=True)

# Replace " - " value as not resign
dfclean['is_resign'] = np.where(dfclean['TanggalResign']=='-',0,1)

# Convert Data Type

dfclean['SkorKepuasanPegawai'] =  dfclean['SkorKepuasanPegawai'].astype('int64')
dfclean['JumlahKeikutsertaanProjek'] = dfclean['JumlahKeikutsertaanProjek'].astype('int64')
dfclean['JumlahKeterlambatanSebulanTerakhir'] = dfclean['JumlahKeterlambatanSebulanTerakhir'].astype('int64')
dfclean['JumlahKetidakhadiran'] = dfclean['JumlahKetidakhadiran'].astype('int64')
dfclean['TanggalLahir'] = pd.to_datetime(dfclean['TanggalLahir'])
dfclean['TanggalHiring'] = pd.to_datetime(dfclean['TanggalHiring'])
dfclean['TanggalPenilaianKaryawan'] = pd.to_datetime(dfclean['TanggalPenilaianKaryawan'])

"""All clear now!

# Data Visualization

## Annual Report on Employee Number
"""

dfviz = dfclean.copy()

# Group by to see total hiring and resign each year

dfviz['TanggalResign'] = pd.to_datetime(dfviz['TanggalResign'], errors='coerce')

hiring_year = dfviz.groupby(dfviz['TanggalHiring'].dt.year)[['Username']].count().reset_index()
resign_year = dfviz.groupby(dfviz['TanggalResign'].dt.year)[['Username']].count().reset_index()
hiring_year.columns=['Year','Hiring_Year']
resign_year.columns=['Year','Resign_Year']

merge = pd.merge(hiring_year, resign_year, how='outer', on='Year').fillna(0).astype('int').sort_values('Year').reset_index(drop=True)

# Total hiring

total_hiring = 0
list_totalhiring = []

for i in range(len(merge)):
    total_hiring += merge['Hiring_Year'][i]
    list_totalhiring.append(total_hiring)

merge['Total_Hiring'] = list_totalhiring

# Total resign

total_resign = 0
list_totalresign = []

for i in range(len(merge)):
    total_resign += merge['Resign_Year'][i]
    list_totalresign.append(total_resign)

merge['Total_Resign'] = list_totalresign

# The changes
merge['Changes'] = merge['Hiring_Year'] - merge['Resign_Year']

# Total Employee

total_employee = 0
list_totalemployee = []

for i in range(len(merge)):
    total_employee += merge['Changes'][i]
    list_totalemployee.append(total_employee)

merge['Total_Employees'] = list_totalemployee
merge

layout = go.Layout(
    title= "Annual Report on Employee Number (2006 - 2021)",
    showlegend=False,
    yaxis=dict(title='Total Employees'),
    xaxis=dict(title='Year')
)

fig = go.Figure(go.Waterfall(
    name = "20", orientation = "v",
    measure = ["relative" for x in range(len(merge['Changes']))]+["total"],
    x = merge['Year'].tolist()+[2021],
    textposition = "outside",
    text = [str(x) if x <=0 else "+"+str(x) for x in merge['Changes']]+[str('Total_Employees')],
    y = merge['Changes'].tolist()+[str('Total_Employees')],
    connector = {"line":{"color": "rgb(63, 63, 63)", "dash":"solid"}},
    increasing = {"marker":{"color":"#55A630"}},
    decreasing = {"marker":{"color":"#C1121F"}},
    totals = {"marker":{"color":"#FFD60A", "line":{"color":"#FFC300", "width":2}}}
    ),
    layout= layout
)

fig.show()
plt.savefig('annual_employee.png')

hiring = [ merge['Hiring_Year'][i] - merge['Hiring_Year'][i-1] for i in range(1,len(merge['Hiring_Year'])) ]
hiring = [1] + hiring

layout = go.Layout(
    title= "Hiring Trend in Company (2006 - 2021)",
    showlegend=False,
    yaxis=dict(title='Total Hired Employees'),
    xaxis=dict(title='Year')
)

go.Figure(go.Waterfall(
    name = "Hiring", orientation = "v",
    x =merge['Year'] ,
    textposition = "auto",
    text = merge['Hiring_Year'],
    y = hiring,
    connector = {"line":{"color":"#55A630"}},
    increasing = {"marker":{"color":"#55A630"}},
    decreasing = {"marker":{"color":"#C1121F"}},
    ),
    layout= layout
)
plt.savefig('hiring_trend.png')

resign = [ merge['Resign_Year'][i] - merge['Resign_Year'][i-1] for i in range(1,len(merge['Resign_Year'])) ]
resign = [1] + resign

layout = go.Layout(
    title= "Resign Trend in Company (2006 - 2021)",
    showlegend=False,
    yaxis=dict(title='Total Resigned Employees'),
    xaxis=dict(title='Year')
)

go.Figure(go.Waterfall(
    name = "Resign", orientation = "v",
    x =merge['Year'] ,
    textposition = "auto",
    text = merge['Resign_Year'],
    y = resign,
    connector = {"line":{"color":"#E01E37"}},
    increasing = {"marker":{"color":"#E01E37"}},
    decreasing = {"marker":{"color":"#99C1DE"}},
    ),
    layout= layout
)
plt.savefig('resign_trend.png')

"""## Resign Reason Analysis"""

# Dataframe for resigned employee
df_resign = dfviz[dfviz['is_resign']==1]
df_resigned = df_resign.groupby('Pekerjaan').agg({'is_resign' : 'count'}).reset_index().rename({'is_resign':'Total_Resign'}, axis=1)
df_resigned.columns = ['Pekerjaan', 'Total_Resign']
df_resigned

# Dataframe for current employee
df_employee = dfviz[dfviz['is_resign']==0]
df_cur_employee = df_employee.groupby('Pekerjaan').agg({'is_resign' : 'count'}).reset_index().rename({'is_resign':'Total_Employee'}, axis=1)
df_cur_employee.columns = ['Pekerjaan', 'Total_Employee']
df_cur_employee

# Join to see ratio between them
df_resign_work = pd.merge(df_resigned,df_cur_employee, left_on='Pekerjaan', right_on='Pekerjaan', how='outer')
df_resign_work['Total_Resign'] = df_resign_work[['Total_Resign']].fillna(0).astype(int)
df_resign_work['Resign(%)'] = round (df_resign_work['Total_Resign'] / (df_resign_work['Total_Resign'] + df_resign_work['Total_Employee']) * 100,2)
df_resign_work

# Visualize the resign reason

sns.set_style('darkgrid')
fig, ax = plt.subplots(figsize=(10, 5))
ax = sns.set_theme(style='darkgrid')
ax = sns.barplot(data = df_resign_work, x='Resign(%)', y='Pekerjaan', palette = 'YlOrRd',
            order = df_resign_work.sort_values('Resign(%)').Pekerjaan)
plt.title('Resigned Employee Ratio', size=15)
plt.xlabel('Resigned Employee (%)', fontsize=11)
plt.ylabel('Occupation', fontsize=11)
plt.xlim(0, 60)
plt.xticks(fontsize=10)
plt.tick_params(axis='both', which='major', labelsize=9)
ax.bar_label(ax.containers[0], fontsize=8)
plt.show()
plt.savefig('resigned_ration.png')

# Visualize total current employee

fig, ax = plt.subplots(figsize=(10, 5))
ax = sns.set_theme(style='darkgrid')
ax = sns.barplot(data = df_resign_work, x='Total_Employee', y='Pekerjaan', palette = 'Spectral',
            order = df_resign_work.sort_values('Total_Employee').Pekerjaan)
plt.title('Total Current Employee by Job Position', size=15)
plt.xlabel('Total Employee', fontsize=11)
plt.ylabel('Job Position', fontsize=11)
plt.xlim(0, 100)
plt.xticks(fontsize=10)
plt.tick_params(axis='both', which='major', labelsize=9)
ax.bar_label(ax.containers[0], fontsize=8)
plt.show()
plt.savefig('current_employee.png')

"""Berdasarkan hasil analisis, pekerjaan Data Analyst merupakan pekerjaan yang paling banyak resign sehingga perlu dilakukan analisa lebih lanjut."""

# Analyze Data Analyst Resign Reason

datanalyst = dfviz[(dfviz['Pekerjaan']=='Data Analyst') & dfviz['is_resign']==1 ].groupby(['Pekerjaan','JenjangKarir', 'PerformancePegawai','AlasanResign']).agg({'is_resign' : 'count'}).reset_index()
datanalyst

"""Terlihat bahwa Data Analyst yang resign semuanya berasal dari Fresh Graduate. Dilihat dari bagaimanapun kinerja pegawai, alasan resign mereka ada 2 yaitu toxic culture (budaya kerja toxic) dan konflik internal."""

# Visualize Data Analyst Resigned Employee

sns.set(style='whitegrid')
fig, ax = plt.subplots(figsize=(8, 5))
plt.title("Data Analyst Resigned Employee", fontsize=15, color='black', weight='bold', pad=65)
plt.text(x=-0.5, y=3.5, s="Data Analyst is the most resigned employee. All of them are from Fresh Graduate Program \nThere are only 2 reasons, they are Toxic Culture and Internal Conflict", fontsize=10, fontstyle='italic')
sns.countplot(x='PerformancePegawai', data=dfclean[(dfclean['Pekerjaan']=='Data Analyst') & dfclean['is_resign']==1],
              hue='AlasanResign', edgecolor= 'green', palette='YlGn')

plt.tick_params(axis='both', which='major', labelsize=9)
plt.grid()
plt.legend(title='Resign Reason', title_fontsize=9, prop={'size':8})
plt.xlabel('Employee Performance', fontsize=11)
plt.xticks(range(0,4,1), labels=['Sangat_bagus', 'Bagus', 'Biasa', 'Sangat_kurang'], fontsize=10)

plt.bar_label(ax.containers[0], padding=5, fontsize=8)
plt.bar_label(ax.containers[1], padding=5, fontsize=8)

sns.despine()
plt.tight_layout()
plt.savefig('datanalyst_resigned.png')

df_resign_all = dfviz.query("is_resign == 1").groupby(["AlasanResign","Pekerjaan"])["EnterpriseID"].count().reset_index()

review_matrix = []
performance_sort = df_resign_all["Pekerjaan"].unique()
for x in df_resign_all["AlasanResign"].unique():
    temp_df = df_resign_all[df_resign_all["AlasanResign"]==x].set_index("Pekerjaan")["EnterpriseID"].reindex(performance_sort).fillna(0)
    temp_vector = [int(x) for x in temp_df.reset_index()["EnterpriseID"]]
    review_matrix.append(temp_vector)

#store as data frame
review_pivot = pd.DataFrame(review_matrix,columns=performance_sort,index=df_resign_all["AlasanResign"].unique())
division_list = np.sum(review_pivot,axis=1).reset_index()[0].tolist()
for index,value in enumerate(review_pivot.index):
    review_pivot.loc[value] = round(100.00*review_pivot.loc[value]/division_list[index],2)

# Create Plot
plt.figure(figsize=(8,5))
ax = sns.heatmap(review_pivot, linewidths=.5, annot=True, fmt=',.02f', cmap="YlGn", cbar=True)
ax.xaxis.set_ticks_position('top')
plt.tick_params(axis='both', which='major', labelsize=9)
plt.rcParams.update({'font.size': 8})
ax.set_xticklabels(ax.get_xticklabels(), rotation = 30, fontsize = 9)
ax.set_ylabel('')
ax.set_xlabel('')
ax.set_title("Percentage of Employee Resignation (%)")
plt.show()
plt.savefig('all_resigned.png')

"""# Feature Engineering"""

dfpre = dfclean.copy()

"""## Feature Extraction

**Age**
"""

# Extract birth year feature

dfpre['BirthYear'] = dfpre['TanggalLahir'].dt.year
dfpre['HiringYear'] = dfpre['TanggalHiring'].dt.year
dfpre['Age'] = dfpre['HiringYear'] - dfpre['BirthYear']

"""**Work Period**"""

dfpre['ResignYear'] = dfpre['TanggalResign'].map(lambda x: x[:4] if x != "-" else "-")
dfpre['WorkPeriod'] = dfpre['ResignYear'].map(lambda x: 0 if x == "-" else x).astype(int) - dfpre['HiringYear'].astype(int)
dfpre['WorkPeriod'] = dfpre['WorkPeriod'].map(lambda x: 0 if x < 0 else x)

dfpre['WorkPeriod'].describe()

"""**Take part of Project**"""

dfpre['TakeProject'] = dfpre['JumlahKeikutsertaanProjek'].map(lambda x: 1 if x!=0 else 0)

"""**Time since last Employee Appraisal**"""

dfpre['EvalYear'] = dfpre['TanggalPenilaianKaryawan'].dt.year
dfpre['SinceLastEval'] = dfpre['EvalYear'] - dfpre['HiringYear']

dfpre[['Age', 'WorkPeriod', 'SinceLastEval']].describe()

dfpre = dfpre.drop(['Username' ,'JenisKelamin',  'AlasanResign','EvalYear', 'HiringYear', 'TanggalLahir','TanggalHiring',
                    'TanggalPenilaianKaryawan','TanggalResign', 'ResignYear', 'BirthYear'],axis=1)
dfpre.sample(3)

"""## Preprocessing"""

dfpre2 = dfpre.copy()

dfpre2.info()

"""## Handle Outliers"""

# Use Boxplot
df_numerical = dfpre2.select_dtypes(include = ["int64","float64"])

# Figure Size Setting
plt.figure(figsize=(10,4))

# Boxplot using Seaborn
sns.boxplot(data=df_numerical.drop(["EnterpriseID"],axis=1), palette='husl')
plt.xticks(rotation=45)
# Show the plot
plt.show()

df_numerical.describe()

"""Untuk `SkorSurveyEngagement`,`JumlahKeikutsertaanProjek`, `JumlahKeterlambatanSebulanTerakhir`, `keikutsertaanproject_boolean`, ditoleransi untuk outliernya. Namun tidak untuk sisanya tidak karena standar deviasi yang tinggi."""

# Function to check for outliers
def outlier_del(df, column, mode):
    q1 = df.iloc[:,column].quantile(0.25)
    q3 = df.iloc[:,column].quantile(0.75)
    iqr = q3-q1
    lower_tail = q1 - (1.5 * iqr)
    upper_tail = q3 + (1.5 * iqr)
    column_name = df.columns[column]
    total_outliers = df[(df.iloc[:,column] <= lower_tail)|(df.iloc[:,column] >= upper_tail)].iloc[:,column].count()
    total_row = df.iloc[:,column].count()
    percent_outliers = round(((total_outliers/total_row)*100),2)
    if mode == 'summary':
        return print('Total Outliers of ', column_name, ' :', total_outliers, ' and outliers percentage:', percent_outliers, '%')
    elif mode == 'df':
        return df[(df.iloc[:,column] >= lower_tail)&(df.iloc[:,column] <= upper_tail)]
    else :
        return print('Check the Input')

# Check the total outliers
column = [9, 12, 15, 18]

for i in range(0, len(column)):
    outlier_del(dfpre2, column[i], 'summary')

dfpre2 = dfpre2[dfpre2.index.isin(outlier_del(dfpre, 12, 'df').reset_index()['index'])]
dfpre2 = dfpre2[dfpre2.index.isin(outlier_del(dfpre, 15, 'df').reset_index()['index'])]
dfpre2 = dfpre2[dfpre2.index.isin(outlier_del(dfpre, 18, 'df').reset_index()['index'])]
dfpre2.shape

"""## Feature Encoding

**Division**
"""

# Working Division
engineering_div = ['Software Engineer (Back End)',
               'Software Engineer (Front End)',
               'Software Engineer (Android)',
               'Software Engineer (iOS)',
               'DevOps Engineer',
               'Software Architect',
               'Machine Learning Engineer']
data_div = ['Data Analyst', 'Data Engineer']
product_div = ['Product Manager',
               'Product Design (UX Researcher)',
               'Product Design (UI & UX)',
               'Digital Product Manager',
               'Scrum Master']

dfpre2['Pekerjaan'] = dfpre2['Pekerjaan'].map(lambda x: 'engineering_div' if x in engineering_div else 'data_div' if x in data_div else 'product_div')

"""**Career Level**"""

# Career level encoding
career_lvl = {'Freshgraduate_program': 1,
              'Mid_level': 2,
              'Senior_level': 3}
dfpre2['JenjangKarir'] = dfpre2['JenjangKarir'].map(career_lvl)

"""**Employee Performance**"""

# Employee performance level
mapping_perform = {
    'Sangat_kurang' : 1,
    'Kurang' : 2,
    'Biasa' : 3,
    'Bagus' : 4,
    'Sangat_bagus' : 5}
dfpre2['PerformancePegawai'] = dfpre2['PerformancePegawai'].map(mapping_perform)

"""**Education Level**"""

mapping_edu = {
    'Sarjana' : 1,
    'Magister' : 2,
    'Doktor' : 3}
dfpre2['TingkatPendidikan'] = dfpre2['TingkatPendidikan'].map(mapping_edu)

dfpre2.rename(columns={'Pekerjaan': 'Job', 'StatusPernikahan': 'Status', 'PerformancePegawai': 'Performance', 'AsalDaerah': 'City',
                       'JenjangKarir' : 'Carrer_lvl', 'TingkatPendidikan' : 'Education', 'StatusKepegawaian' : 'Emp_Status'}, inplace=True)

dfpre2.sample(3)

"""**Frequency Encoding**"""

# Frequency Encoding
freq_code = dfpre2.groupby('HiringPlatform').size()/len(dfpre2)
dfpre2['Hiring_Platform'] = dfpre2["HiringPlatform"].map(freq_code)
dfpre2.drop(["HiringPlatform"],axis=1,inplace=True)

"""**One-Hot Encoding**"""

# One Hot Encoding

dfpre2 = pd.get_dummies(dfpre2, columns=['Job', 'Status', 'City', 'Emp_Status'])
dfpre2.sample(3)

dfpre2.info()

"""## Handle Imbalance"""

dfnew = dfpre2.copy().set_index(['EnterpriseID'])
dfnew.sample(3)

# Check feature target

dfnew['is_resign'].value_counts()

from sklearn.model_selection import train_test_split

# Split the data
features = dfnew.select_dtypes(["int64", "float64", "uint8"]).columns
x = dfnew[features].drop('is_resign', axis=1)
y = dfnew['is_resign'].tolist()

xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.2)

from imblearn.under_sampling import RandomUnderSampler, TomekLinks, EditedNearestNeighbours
from imblearn.combine import SMOTEENN, SMOTETomek

from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.datasets import make_classification
from imblearn.pipeline import Pipeline
from sklearn.model_selection import cross_validate
from IPython.display import clear_output

tl = TomekLinks(sampling_strategy="auto")
rus = RandomUnderSampler(random_state=123)
enn = EditedNearestNeighbours()
senn = SMOTEENN(random_state=123)
stl = SMOTETomek(random_state=123)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier,BaggingClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

from sklearn.metrics import accuracy_score,roc_auc_score,precision_score,recall_score,f1_score,log_loss
from sklearn.metrics import confusion_matrix, classification_report

# Thresholding Imbalanced Learning using ML Metrics

list_all_imbalanced = [tl,rus,enn,senn,stl]

imbalance_strategy = []
score_accuracy = []
score_precision = []
score_recall = []
score_auc = []
time_training = []

for imbalanced_learning in list_all_imbalanced:
    print(f"Processing {imbalanced_learning.__class__.__name__}")
    start_time = time.time()
    model = XGBClassifier(verbosity = 0,use_label_encoder = False, random_state = 123)
    #Define pipeline
    pipeline=Pipeline(steps=[("r", imbalanced_learning), ("m", model)])

    #Define evaluation procedure (here we use Repeated Stratified K-Fold CV)
    cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)

    # Evaluate model
    scoring=["accuracy","precision_macro","recall_macro","roc_auc"]
    scores = cross_validate(pipeline, xtrain, ytrain, scoring=scoring, cv=cv, n_jobs=-1)

    imbalance_strategy.append(imbalanced_learning.__class__.__name__)
    score_accuracy.append(np.mean(scores["test_accuracy"]))
    score_precision.append(np.mean(scores["test_precision_macro"]))
    score_recall.append(np.mean(scores["test_recall_macro"]))
    score_auc.append(np.mean(scores["test_roc_auc"]))
    end_time = time.time()
    time_training.append(time.strftime('%H:%M:%S', time.gmtime(end_time - start_time)))

# Result
clear_output()
df_evaluation = pd.DataFrame({"Imbalance_Strategy":imbalance_strategy,
                              "Accuracy": score_accuracy,
                              "Precision": score_precision,
                              "Recall": score_recall,
                              "AUC": score_auc,
                              "Training_Time": time_training})
df_evaluation.sort_values(["AUC"],ascending=False)

"""TomekLinks menunjukkan yang paling baik diantara lainnya

# Modeling
"""

#Initialize the classifier model
model1 = GaussianNB()
model2 = SVC(random_state=0)
model3 = RandomForestClassifier(random_state=0)
model4 = LogisticRegression()
model5 = DecisionTreeClassifier(random_state=0)
model6 = BaggingClassifier(random_state=0,base_estimator=model5)
model7 = GradientBoostingClassifier(random_state=0)
model8 = AdaBoostClassifier(random_state=0,base_estimator=model5)
model9 = KNeighborsClassifier()
model10 = MLPClassifier()
model11 = XGBClassifier(random_state=0)
model12 = LGBMClassifier(random_state=0)
model13 = CatBoostClassifier(random_state=0)

classifier_list = [model1,model2,model3,model4,model5,model6,model7,model8,model9,model10,model11,model12, model13]

ml_model = []
score_accuracy = []
score_precision = []
score_recall = []
score_auc = []
time_training = []

for model in classifier_list:
    print(f"Processing {model.__class__.__name__}")

    start_time = time.time()
    tl = TomekLinks(sampling_strategy="auto")

    #Define pipeline
    pipeline=Pipeline(steps=[("r", tl), ("m", model)])

    #Define evaluation procedure (here we use Repeated Stratified K-Fold CV)
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)

    # Evaluate model
    scoring=["accuracy","precision_macro","recall_macro","roc_auc"]

    scores = cross_validate(pipeline,
                            xtrain,
                            ytrain,
                            scoring = scoring,
                            cv = cv,
                            n_jobs = -1)

    ml_model.append(model.__class__.__name__)
    score_accuracy.append(np.mean(scores["test_accuracy"]))
    score_precision.append(np.mean(scores["test_precision_macro"]))
    score_recall.append(np.mean(scores["test_recall_macro"]))
    score_auc.append(np.mean(scores["test_roc_auc"]))
    end_time = time.time()
    time_training.append(time.strftime('%H:%M:%S', time.gmtime(end_time - start_time)))

# Result
clear_output()
df_model = pd.DataFrame({"ML_Model":ml_model,
                         "Accuracy": score_accuracy,
                         "Precision": score_precision,
                         "Recall": score_recall,
                         "AUC": score_auc,
                         "Training_Time": time_training})
df_model.sort_values(["AUC"],ascending=False)

"""### Tuning Hyperparameter

**CatBoostClassifier**
"""

from hyperopt import hp, tpe, STATUS_OK, Trials
from hyperopt.pyll.stochastic import sample

from timeit import default_timer as timer
from sklearn.model_selection import cross_val_score

# TomekLinks Application
tl = TomekLinks(sampling_strategy="auto")

x_tl, y_tl = tl.fit_resample(xtrain, ytrain)

# Hyperparameter Space
space = {
    'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),
    'n_estimators': hp.quniform('n_estimators',100,500,20),
    "max_depth" : hp.quniform('max_depth', 3, 10, 1)
}

def rskf_metrics(model, X, y, scoring_used = "precision", cv_method = "", nfolds = 10):
    if cv_method == "":
        cv_method = RepeatedStratifiedKFold(n_splits=nfolds, n_repeats=3, random_state=1)

    metrics_calculation= cross_val_score(model, X, y, scoring=scoring_used, cv = cv_method)

    return(metrics_calculation)

def objective(params,
              n_folds=10,
              X=x_tl,
              y=y_tl,
              scoring_used="precision",
              cv_used=""):

    global ITERATION
    ITERATION += 1

    # Perform n_folds cross validation
    start = timer()
    model_hyper = CatBoostClassifier(random_state = 123,verbose=0, **params)
    run_time = timer() - start

    metrics_used = np.mean(rskf_metrics(model_hyper, X, y))

    # Dictionary with information for evaluation
    return {'loss': metrics_used, 'params': params, 'iteration': ITERATION,
            'train_time': run_time, 'status': STATUS_OK}

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from hyperopt import fmin
# 
# # Global variable
# global  ITERATION
# 
# ITERATION = 0
# 
# # optimization algorithm
# tpe_algorithm = tpe.suggest
# 
# # Hyperopts Trials() records all the model and run artifacts.
# trials = Trials()
# 
# # Fmin will call the objective funbction with selective param set.
# # The choice of algorithm will narrow the searchspace.
# 
# best_classifier = fmin(objective, space, algo=tpe_algorithm,
#                        max_evals=100, trials=trials)

from hyperopt import space_eval

# Best_params of the best model
best_params = space_eval(space, best_classifier)
best_params

catboost_model = CatBoostClassifier(random_state = 123, **best_classifier)
catboost_model.fit(x_tl,y_tl,verbose=0)

y_pred = catboost_model.predict(xtest)
y_pred_proba = catboost_model.predict_proba(xtest)

from sklearn.metrics import roc_curve, roc_auc_score, auc

def roc_plot(clf, X_test,y_test):
    probs = clf.predict_proba(X_test)
    preds = probs[:,1]
    fpr, tpr, threshold = roc_curve(y_test, preds)
    roc_auc = auc(fpr, tpr)

    plt.title('Receiver Operating Characteristic')
    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    plt.show()

roc_plot(catboost_model,xtest,ytest)

"""**GaussianNB**"""

from sklearn.model_selection import GridSearchCV

# TomekLinks Application
tl = TomekLinks(sampling_strategy="auto")

x_tl, y_tl = tl.fit_resample(xtrain, ytrain)

params_NB = {'var_smoothing': np.logspace(0,-9, num=100)}
gs_NB = GridSearchCV(estimator=model1,
                 param_grid=params_NB,
                 cv=RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1),   # use any cross validation technique
                 verbose=1,
                 scoring='precision')
gs_NB.fit(x_tl, y_tl)

gs_NB.best_params_

gnb_model = GaussianNB(var_smoothing = 0.231)
gnb_model.fit(x_tl,y_tl)

y_pred = gnb_model.predict(xtest)
y_pred_proba = gnb_model.predict_proba(xtest)

from sklearn.metrics import roc_curve, roc_auc_score, auc

def roc_plot(clf, X_test,y_test):
    probs = clf.predict_proba(X_test)
    preds = probs[:,1]
    fpr, tpr, threshold = roc_curve(y_test, preds)
    roc_auc = auc(fpr, tpr)

    plt.title('Receiver Operating Characteristic')
    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
    plt.legend(loc = 'lower right')
    plt.plot([0, 1], [0, 1],'r--')
    plt.xlim([0, 1])
    plt.ylim([0, 1])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    plt.show()

roc_plot(gnb_model,xtest,ytest)

"""**RandomForestClassifier**"""

# Fit the grid search to the data
from sklearn.model_selection import RandomizedSearchCV

# TomekLinks Application
tl = TomekLinks(sampling_strategy="auto")

x_tl, y_tl = tl.fit_resample(xtrain, ytrain)

rf_grid = {"n_estimators": np.arange(10, 100, 10),
           "max_depth": [None, 3, 5, 10],
           "min_samples_split": np.arange(2, 20, 2),
           "min_samples_leaf": np.arange(1, 20, 2),
           "max_features": [0.5, 1, "sqrt", "auto"]}

# Instantiate RandomizedSearchCV model
rf_model = RandomizedSearchCV(RandomForestClassifier(n_jobs=-1, random_state=42),
                              param_distributions=rf_grid,
                              n_iter=2,
                              cv=5,
                              verbose=True)
# fit
rf_model.fit(x_tl, y_tl)

print(rf_model.best_params_)

rfc_model = RandomForestClassifier(n_jobs = -1, random_state=42)
rfc_model.fit(x_tl,y_tl)

y_pred = rfc_model.predict(xtest)
y_pred_proba = rfc_model.predict_proba(xtest)

roc_plot(rfc_model,xtest,ytest)

roc_plot(rfc_model,xtrain,ytrain)

# Confusion Matrix Random Forest

predictions_XGB = rfc_model.predict(xtest)
cf_matrix = confusion_matrix(predictions_XGB, ytest)

group_names = ['TN','FP','FN','TP']
group_counts = ['{0:0.0f}'.format(value) for value in
                cf_matrix.flatten()]
group_percentages = ['{0:.2%}'.format(value) for value in
                     cf_matrix.flatten()/np.sum(cf_matrix)]
labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in
          zip(group_names,group_counts,group_percentages)]
labels = np.asarray(labels).reshape(2,2)

fig, ax = plt.subplots(figsize=(5, 4))
plt.tick_params(axis='both', which='major', labelsize=8)
plt.rcParams.update({'font.size': 8})
ax = sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Reds')

plt.title('RF\nConfusion Matrix', pad=15)
plt.xlabel('Predicted Values')
plt.ylabel('Actual Values ');
plt.savefig('RFC ConfMtrx', dpi = 200)

"""Dari hasil percobaan tuning hyperparameter terhadap 3 model skor metriks yang tinggi, terlihat bahwa model RandomForest memiliki performa yang terbaik. Hasil train dan test yang bernilai 1.00 dan 0.99 mungkin saja terjadi karena data yang dimiliki hanya sedikit, yaitu sampel 20% dari 272. Adapun jika data yang dimiliki lebih banyak, kemungkinan skor juga bisa saja berkurang."""

# Displays a feature importance graph

# Training model xgboost
modelrf = RandomForestClassifier()
modelrf.fit(xtrain, ytrain)

feat_importances = pd.Series(modelrf.feature_importances_, index=x.columns)
ax = feat_importances.nlargest(15).plot(kind='barh')
ax.invert_yaxis()
plt.xlabel('score')
plt.ylabel('feature')
plt.title('Feature Importance Score')

from sklearn.inspection import PartialDependenceDisplay

fig, ax = plt.subplots(figsize=(12, 6))
ax.set_ylim(100,200)

tree_disp = PartialDependenceDisplay.from_estimator(rfc_model, x_tl, ["WorkPeriod", "Age", "JumlahKetidakhadiran", "SinceLastEval"], ax=ax)
tree_disp.axes_[0][0]
tree_disp.axes_[0][1]
tree_disp.axes_[1][1]
tree_disp.axes_[1][0]

"""Dari grafik feature importance diatas, lama tahun kerja pegawai menunjukkan kepentingan yang sangat tinggi. Hal ini menunjukkan bahwa semakin lama seseorang sudah bekerja di perusahaan (dalam hitungan tahun) maka resiko untuk resign juga semakin tinggi. Faktor selanjutnya yaitu jarak waktu terakhir penilaian terhadap karyawan. Semakin jauh jarak nya, semakin cenderung pegawai tersebut memutuskan untuk resign. Faktor umur juga berpengaruh terhadap kecenderungan pegawai dalam resign.

Hal tersebut menunjukkan bahwa kondisi kesehatan pegawai perusahaan sedang tidak baik-baik saja. Bukannya semakin betah di perusahaan, namun pegawai memilih untuk resign.

Recommendation :

1. Melakukan evaluasi rutinan terhadap kepuasan pegawai ditinjau dari budaya perusahaan dan hubungan dengan rekan kerja.  Hal ini terlihat dari alasan terbesar banyak pegawai yang resign, yaitu toxic culture dan conflict internal.  

2. Perusahaan menyediakan fasilitas terapi mental (mendatangkan psikolog atau psikiater) sebagai wadah pegawai dalam mengeluarkan isi hatinya.  

3. Menumbuhkan budaya saling mengapresiasi sesama rekan kerja agar pegawai tidak merasa hanya bekerja dalam tekanan.
"""